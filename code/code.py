# -*- coding: utf-8 -*-
"""PS2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VX51dDU_Nx8ettFiH42sxwsBsmER51nh
"""

# -*- coding: utf-8 -*-
"""
PS2-Style Improved Notebook with SHAP Update
============================================
This notebook builds on the PS1 workflow by adding:
1. Hyperparameter tuning (GridSearchCV) for Logistic Regression, Decision Tree, and Random Forest.
2. Fairness metrics and interpretability (SHAP), using TreeExplainer for Random Forest to avoid
   the additivity check error.
3. Neural network model with dropout for additional predictive experimentation.
4. Placeholder for Regression Discontinuity (RD) in a separate notebook (rd_analysis.ipynb).

Author: [Your Name]
"""

# 1️⃣ Install & Import Required Libraries

!pip install shap tensorflow scikit-learn pandas numpy matplotlib seaborn xgboost

import pandas as pd
import numpy as np
import shap
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

# (Optional) XGBoost or other advanced models
import xgboost as xgb

# Initialize SHAP for JS-based visualization in notebooks
shap.initjs()

# 2️⃣ Load and Preprocess Dataset

file_path = "/content/220831_STATE OF HIRING DISCRIMINATION.xlsx"
xls = pd.ExcelFile(file_path)
df = xls.parse('register')

# Select relevant columns
columns_to_keep = [
    "ground", "study", "treatment_group_high", "control_group",
    "callback_maj", "callback_min", "effect_category"
]
df = df[columns_to_keep].dropna()

# Encode categorical variables
df["effect_category_encoded"] = df["effect_category"].astype("category").cat.codes
df["treatment_group_encoded"] = df["treatment_group_high"].astype("category").cat.codes
df["control_group_encoded"] = df["control_group"].astype("category").cat.codes

# Compute callback rate difference
df["callback_diff"] = df["callback_maj"] - df["callback_min"]

# Define features & target
X = df[["treatment_group_encoded", "control_group_encoded", "callback_diff"]]
y = df["effect_category_encoded"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Standardize
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Data Preview:")
display(df.head())

# 3️⃣ Exploratory Data Analysis (EDA)

plt.figure(figsize=(10, 5))
sns.barplot(x=df["treatment_group_high"], y=df["callback_diff"], palette="coolwarm")
plt.xlabel("Treatment Group")
plt.ylabel("Callback Rate Difference")
plt.title("Callback Rate Disparities by Treatment Group")
plt.xticks(rotation=45)
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

# 4️⃣ Hyperparameter Tuning and Model Training

## 4.1 Logistic Regression with GridSearchCV
param_grid_log = {
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l2']  # 'l1' if solver supports it
}
log_base = LogisticRegression(max_iter=1000, random_state=42)
grid_log = GridSearchCV(
    log_base,
    param_grid_log,
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    scoring='accuracy',
    n_jobs=-1
)
grid_log.fit(X_train_scaled, y_train)

best_log_model = grid_log.best_estimator_
y_pred_logistic = best_log_model.predict(X_test_scaled)

print("=== Logistic Regression (Tuned) ===")
print(f"Best Params: {grid_log.best_params_}")
print(f"Accuracy: {accuracy_score(y_test, y_pred_logistic):.4f}")
print(classification_report(y_test, y_pred_logistic))

## 4.2 Decision Tree with GridSearchCV
param_grid_dt = {
    'max_depth': [3, 5, 7, None],
    'min_samples_leaf': [1, 2, 5]
}
dt_base = DecisionTreeClassifier(random_state=42)
grid_dt = GridSearchCV(
    dt_base,
    param_grid_dt,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid_dt.fit(X_train_scaled, y_train)

best_dt_model = grid_dt.best_estimator_
y_pred_dt = best_dt_model.predict(X_test_scaled)

print("\n=== Decision Tree (Tuned) ===")
print(f"Best Params: {grid_dt.best_params_}")
print(f"Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}")
print(classification_report(y_test, y_pred_dt))

## 4.3 Random Forest with GridSearchCV
param_grid_rf = {
    'n_estimators': [50, 100],
    'max_depth': [3, 5, 7],
    'min_samples_leaf': [1, 2, 5]
}
rf_base = RandomForestClassifier(random_state=42)
grid_rf = GridSearchCV(
    rf_base,
    param_grid_rf,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid_rf.fit(X_train_scaled, y_train)

best_rf_model = grid_rf.best_estimator_
y_pred_rf = best_rf_model.predict(X_test_scaled)

print("\n=== Random Forest (Tuned) ===")
print(f"Best Params: {grid_rf.best_params_}")
print(f"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}")
print(classification_report(y_test, y_pred_rf))

# (Optional) 4.4 XGBoost or other advanced models
param_grid_xgb = {
    'learning_rate': [0.01, 0.1],
    'max_depth': [3, 5],
    'n_estimators': [50, 100]
}
xgb_base = xgb.XGBClassifier(eval_metric='mlogloss', random_state=42)
grid_xgb = GridSearchCV(
    xgb_base,
    param_grid_xgb,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid_xgb.fit(X_train_scaled, y_train)
best_xgb_model = grid_xgb.best_estimator_
y_pred_xgb = best_xgb_model.predict(X_test_scaled)
print("\n=== XGBoost (Tuned) ===")
print(f"Best Params: {grid_xgb.best_params_}")
print(f"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}")
print(classification_report(y_test, y_pred_xgb))

# 5️⃣ Fairness Metrics

print("\n=== Fairness Metrics ===")
demographic_parity = df.groupby("treatment_group_encoded")["callback_diff"].mean()
disparate_impact_ratio = df["callback_min"].mean() / df["callback_maj"].mean()
print(f"Demographic Parity (by 'treatment_group_encoded'):\n{demographic_parity}")
print(f"Disparate Impact Ratio: {disparate_impact_ratio:.4f}")

# Let's pick the best performing model (e.g., Random Forest) to compute partial E.Odds
cm_rf = confusion_matrix(y_test, y_pred_rf)
tpr_rf = cm_rf[1,1] / (cm_rf[1,1] + cm_rf[1,0]) if (cm_rf[1,1] + cm_rf[1,0]) > 0 else 0
fpr_rf = cm_rf[0,1] / (cm_rf[0,1] + cm_rf[0,0]) if (cm_rf[0,1] + cm_rf[0,0]) > 0 else 0
print(f"Random Forest TPR: {tpr_rf:.4f}, FPR: {fpr_rf:.4f}")

# 6️⃣ SHAP Interpretability on Random Forest
print("\n=== SHAP Interpretability ===")
# Using TreeExplainer specifically for Random Forest to avoid additivity error
rf_explainer = shap.TreeExplainer(best_rf_model)
rf_shap_values = rf_explainer.shap_values(X_test_scaled)

plt.figure(figsize=(10, 6))
shap.summary_plot(rf_shap_values, X_test_scaled, feature_names=X.columns)
plt.show()

# 7️⃣ Visualization of Fairness Metrics
plt.figure(figsize=(10, 5))
sns.barplot(x=demographic_parity.index, y=demographic_parity.values, palette="coolwarm")
plt.xlabel("Treatment Group (Encoded)")
plt.ylabel("Avg Callback Rate Difference")
plt.title("Demographic Parity: Avg Callback Rate Diff per Group")
plt.xticks(rotation=45)
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

plt.figure(figsize=(5, 4))
plt.bar(["Disparate Impact Ratio"], [disparate_impact_ratio], color="steelblue")
plt.axhline(y=0.8, color="red", linestyle="--", label="Adverse Impact Threshold (0.8)")
plt.ylabel("Ratio")
plt.title("Disparate Impact Ratio")
plt.legend()
plt.ylim(0, 1.2)
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

# 8️⃣ Neural Network Training
print("\n=== Neural Network Training ===")
nn_model = Sequential([
    Dense(16, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    Dropout(0.3),
    Dense(8, activation='relu'),
    Dense(len(np.unique(y_train)), activation='softmax')
])
nn_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

history = nn_model.fit(
    X_train_scaled, y_train,
    epochs=30, batch_size=8,
    validation_data=(X_test_scaled, y_test),
    verbose=1
)

nn_loss, nn_accuracy = nn_model.evaluate(X_test_scaled, y_test)
print(f"Neural Network Accuracy: {nn_accuracy:.4f}")


# (Optional) SHAP for Neural Network
# For large datasets or big models, Kernel SHAP can be slow. Example:
explainer_nn = shap.KernelExplainer(nn_model.predict, X_train_scaled[:50])
shap_values_nn = explainer_nn.shap_values(X_test_scaled[:50])
plt.figure(figsize=(10, 6))
shap.summary_plot(shap_values_nn, X_test_scaled[:50], feature_names=X.columns)
plt.show()

# 9️⃣ Placeholder: Regression Discontinuity or Additional Analyses
"""
In a separate 'rd_analysis.ipynb', we would:

1. Load or simulate a dataset with a running variable (e.g., skill_score) and a cutoff=80.
2. Label treatment if skill_score >= 80.
3. Perform local linear regression or other RD methods to estimate the jump at the cutoff.
4. Conduct robustness checks (placebo cutoffs, bandwidth changes).

This completes the causal inference component of PS2.
"""

print("\n--- End of Updated Notebook ---")
print("We've added hyperparameter tuning, advanced interpretability (TreeExplainer), and placeholders for RD.")
